# Sprint 3: Processing & Storage Layer Implementation Plan

## ðŸŽ¯ Goal

Implement Spark Structured Streaming processor with Bronze/Silver/Gold data layers and Cassandra persistence.

---

## âœ… Current State (After Sprint 2)

| Component | Status |
|-----------|--------|
| Finnhub WebSocket Client | âœ… Implemented |
| Kafka Producer | âœ… Implemented |
| Data Validation | âœ… Implemented |
| Pipeline Orchestrator | âœ… Implemented |
| Docker Infrastructure | âœ… Configured |
| Unit Tests | âœ… 42 tests passing |

---

## ðŸ“ Proposed Changes

### Component 1: Spark Consumer

#### [MODIFY] `src/consumer/spark_processor.py`
Enhance existing processor with:
- Kafka source configuration
- Bronze layer (raw data â†’ Cassandra)
- Silver layer (cleaned, deduplicated)
- Gold layer (5-min OHLCV aggregations)

#### [NEW] `src/consumer/transformations.py`
Data transformation functions:
- `parse_trade_from_avro()` - Deserialize Kafka messages
- `calculate_ohlcv()` - OHLC + Volume aggregation
- `apply_watermark()` - Handle late data

#### [NEW] `src/consumer/sinks.py`
Output sink configurations:
- `CassandraSink` - Write to Cassandra tables
- `ConsoleSink` - Debug output

---

### Component 2: Cassandra Integration

#### [MODIFY] `src/storage/cassandra_client.py`
- Connection pool management
- Prepared statements for inserts
- TTL-based data expiration

#### [NEW] `scripts/init_cassandra.py`
- Creates keyspace and tables
- Applies schema from `schemas/cassandra/keyspace.cql`

---

### Component 3: Spark Job Runner

#### [NEW] `src/consumer/main.py`
Entry point for Spark job:
- CLI arguments for job selection
- Log level configuration
- Checkpoint management

#### [MODIFY] `docker-compose.yml`
- Add Spark worker configuration
- Mount volume for checkpoints
- Configure Cassandra networking

---

## ðŸ§ª Verification Plan

### Automated Tests

| Test | Command |
|------|---------|
| Unit tests | `pytest tests/unit/test_transformations.py` |
| Integration | `pytest tests/integration/test_spark_cassandra.py` |

### Manual Verification

1. **Start all services**
   ```bash
   docker-compose up -d
   ```

2. **Initialize Cassandra schema**
   ```bash
   python scripts/init_cassandra.py
   ```

3. **Start producer pipeline**
   ```bash
   python -m src.producer.main --symbols AAPL,MSFT
   ```

4. **Submit Spark job**
   ```bash
   docker-compose exec spark-master spark-submit \
     --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
     /app/src/consumer/main.py
   ```

5. **Verify data in Cassandra**
   ```bash
   docker-compose exec cassandra cqlsh -e \
     "SELECT * FROM market_data.trades_bronze LIMIT 10;"
   ```

---

## âš ï¸ User Review Required

> [!IMPORTANT]
> **Spark Dependencies**: The Spark job requires Kafka and Cassandra connector JARs. These will be configured in `docker-compose.yml`.

> [!NOTE]
> **Data Flow**: Finnhub â†’ Kafka â†’ Spark â†’ Cassandra. All layers run continuously with checkpointing for fault tolerance.

---

## ðŸ“Š Task Breakdown

| Task | Agent | Priority | Est. Time |
|------|-------|----------|-----------|
| Implement `transformations.py` | backend-specialist | ðŸ”´ Critical | 2h |
| Implement `sinks.py` | backend-specialist | ðŸ”´ Critical | 1.5h |
| Update `spark_processor.py` | backend-specialist | ðŸ”´ Critical | 2h |
| Create `src/consumer/main.py` | backend-specialist | ðŸŸ¡ High | 1h |
| Create `init_cassandra.py` | devops-specialist | ðŸŸ¡ High | 0.5h |
| Update `cassandra_client.py` | backend-specialist | ðŸŸ¡ High | 1h |
| Unit tests for transformations | testing-specialist | ðŸŸ¡ High | 1.5h |
| Integration test | testing-specialist | ðŸŸ¡ High | 1.5h |
| Update Docker Compose | devops-specialist | ðŸŸ¢ Medium | 0.5h |

**Total Estimated Time**: ~12 hours

---

## ðŸ—ï¸ Data Layer Architecture

```mermaid
flowchart TB
    subgraph Kafka
        T1[trades_raw topic]
    end
    
    subgraph Spark["Spark Streaming"]
        B[Bronze Layer]
        S[Silver Layer]
        G[Gold Layer]
    end
    
    subgraph Cassandra
        TB[(trades_bronze)]
        TS[(trades_silver)]
        TG5[(trades_gold_5m)]
        TG1H[(trades_gold_1h)]
    end
    
    T1 --> B
    B --> TB
    B --> S
    S --> TS
    S --> G
    G --> TG5
    G --> TG1H
```

---

## âœ… Definition of Done

- [x] Spark job reads from Kafka `trades_raw` topic
- [x] Bronze layer persists raw trades to Cassandra
- [x] Silver layer deduplicates and cleans data
- [x] Gold layer produces 5-min OHLCV aggregations
- [x] Watermarking handles late data (10-min threshold)
- [x] All tests pass (>80% coverage)
- [x] End-to-end data flow verified

---

*Plan created following `/plan` workflow*
