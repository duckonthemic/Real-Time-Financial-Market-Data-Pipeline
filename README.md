# Real-Time Financial Market Data Pipeline

A scalable, real-time data pipeline for ingesting, processing, and visualizing stock market data from Finnhub.

## ğŸ—ï¸ Architecture

```
Finnhub WebSocket â†’ Python Producer â†’ Kafka â†’ Spark Streaming â†’ Cassandra â†’ Grafana
```

| Layer | Technology | Purpose |
|-------|------------|---------|
| Source | Finnhub WebSocket | Live market data streaming |
| Ingestion | Python + websockets | Data collection |
| Buffering | Apache Kafka + Avro | Message queue with schema |
| Processing | Spark Structured Streaming | Real-time transformations |
| Storage | Apache Cassandra | Time-series persistence |
| Visualization | Grafana | Real-time dashboards |

## ğŸ“ Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ producer/         # Finnhub â†’ Kafka
â”‚   â”œâ”€â”€ consumer/         # Spark processing jobs
â”‚   â”œâ”€â”€ storage/          # Cassandra utilities
â”‚   â””â”€â”€ utils/            # Shared utilities
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ avro/             # Kafka message schemas
â”‚   â””â”€â”€ cassandra/        # CQL table definitions
â”œâ”€â”€ grafana/              # Dashboard configurations
â”œâ”€â”€ docker/               # Service Dockerfiles
â”œâ”€â”€ tests/                # Unit & integration tests
â”œâ”€â”€ docs/                 # Documentation
â”œâ”€â”€ config/               # Environment configs
â”œâ”€â”€ docker-compose.yml    # Service orchestration
â”œâ”€â”€ Makefile              # Common commands
â””â”€â”€ requirements.txt      # Python dependencies
```

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- Python 3.10+
- Finnhub API key (free tier: https://finnhub.io/)

### 1. Clone and Configure

```bash
# Clone the repository
git clone <repository-url>
cd finnhub-realtime-pipeline

# Copy environment template
cp config/dev.env .env

# Add your Finnhub API key
echo "FINNHUB_API_KEY=your_key_here" >> .env
```

### 2. Start Services

```bash
# Start all Docker services
make docker-up

# Or using docker-compose directly
docker-compose up -d
```

### 3. Initialize Infrastructure

```bash
# Create Kafka topics
make topics

# Initialize Cassandra schema
make cassandra-init
```

### 4. Run the Pipeline

```bash
# Install Python dependencies
make install

# Start the producer (ingests from Finnhub)
make producer

# In another terminal, start Spark job
make spark-job
```

### 5. Access Dashboards

| Service | URL | Credentials |
|---------|-----|-------------|
| Grafana | http://localhost:3000 | admin / admin |
| Spark UI | http://localhost:8080 | - |
| Kafka UI | http://localhost:8090 | - |

## ğŸ› ï¸ Development

### Setup Development Environment

```bash
make dev
```

### Run Tests

```bash
# All tests
make test

# Unit tests only
make test-unit

# Integration tests
make test-integration
```

### Code Quality

```bash
# Format code
make format

# Run linters
make lint

# Run all checks
make check
```

## ğŸ“Š Data Layers

| Layer | Table | TTL | Purpose |
|-------|-------|-----|---------|
| Bronze | trades_bronze | 7 days | Raw audit trail |
| Silver | trades_silver | 30 days | Cleaned data |
| Gold | trades_gold_5m | 90 days | 5-min OHLCV |
| Gold | trades_gold_1h | 365 days | 1-hour OHLCV |

## âš ï¸ Finnhub Free Tier Limits

- 1 WebSocket connection per API key
- Max 50 symbols per connection
- 30 API calls/second
- Some symbols may return volume=0

## ğŸ› Troubleshooting

### Services not starting
```bash
docker-compose logs <service-name>
```

### Kafka consumer lag
```bash
docker-compose exec kafka kafka-consumer-groups \
  --bootstrap-server localhost:9092 \
  --describe --group <group-id>
```

### Cassandra connection issues
```bash
docker-compose exec cassandra nodetool status
```

## ğŸ“š Documentation

- [System Analysis & Architecture](PLAN/SYSTEM_ANALYSIS.md)
- [Deployment Guide](docs/DEPLOYMENT.md)
- [Operational Runbook](docs/RUNBOOK.md)
- [Research Findings](docs/research/)

## ğŸ§ª Verify Installation

```bash
# Run smoke test
python scripts/smoke_test.py -v
```

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.
